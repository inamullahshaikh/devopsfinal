# Observability: Fundamentals, Telemetry Signals, and Tools

## Module 1: Observability Fundamentals

Observability is **the ability to assess a system’s current state from the data it produces**. In practice, it means gathering and analyzing telemetry – primarily **metrics**, **logs**, and **traces** – to understand the internal behavior of complex, distributed systems. The term originates from control theory and has become essential in cloud-native and microservices environments. For example, one report notes that developers still spend roughly **50% of their time debugging**, often due to inadequate visibility into system behavior. Observability tools aim to reduce this debugging burden by enabling end-to-end visibility: monitoring key performance indicators, detecting anomalies, tracing request flows, and correlating data before issues impact users.

Real-world examples illustrate its importance. For instance, if an e-commerce site experiences slow checkouts, observability data might reveal **metrics** (e.g. high CPU or request latency), **logs** (e.g. database timeout errors), and **traces** (e.g. a slow call to an external payment service), helping engineers pinpoint the root cause and resolve it.

### Telemetry Pillars: Metrics, Logs, and Traces

Observability relies on three main types of telemetry data – often called the “three pillars” of observability:

* **Metrics** are numeric, time-series measurements (e.g. CPU load, request rate). They summarize system performance over time and help answer “what” is happening.
* **Logs** are timestamped records of discrete events or messages (e.g. error messages, transactions) generated by applications or infrastructure. Logs provide context and detail and help explain **why** something happened.
* **Traces** capture the end-to-end flow of individual requests or transactions across services, showing **how** a particular operation proceeds through the system.

Each pillar complements the others: metrics show trends and anomalies, logs give human-readable context, and traces reveal the path and latency of requests. As the \[Observability vs Monitoring] article explains, **“Monitoring tells us what is happening,” while “Observability explains why it is happening.”** In other words, **monitoring** (often based on metrics and alerts) can detect that something is wrong (e.g. a spike in errors), but **observability** combines metrics, logs, and traces to drill down into root causes.

&#x20;*Figure: Illustration of Monitoring vs Observability (monitoring shows surface symptoms; observability uncovers hidden causes).*

| Telemetry Type | What It Measures                                             | Example Tools                   | Role in Observability                                                     |
| -------------- | ------------------------------------------------------------ | ------------------------------- | ------------------------------------------------------------------------- |
| **Metrics**    | Quantitative time-series data (e.g. CPU%, request latency)   | Prometheus, Graphite, Datadog   | Show overall system health and trends (“what” is happening)               |
| **Logs**       | Timestamped event records (e.g. error messages, access logs) | Grafana Loki, ELK Stack, Splunk | Provide context/detail for events; explain “why” things happen            |
| **Traces**     | Distributed request flows/spans across services              | OpenTelemetry, Jaeger, Zipkin   | Reveal the sequence and timing of operations (“how” a request propagates) |

#### Monitoring vs. Observability

Though related, **monitoring** and **observability** are distinct. Monitoring is the practice of actively collecting data and alerting on known conditions (e.g. “CPU > 90%” triggers an alert). Observability is broader: it includes monitoring but also embraces the ability to diagnose unknown issues by correlating logs, metrics, and traces. Put simply, monitoring answers **“Is something wrong?”**, while observability answers **“Why is it wrong?”**.

| Aspect         | Monitoring                                                      | Observability                                                                   |
| -------------- | --------------------------------------------------------------- | ------------------------------------------------------------------------------- |
| **Goal**       | Detect and alert on predefined issues (thresholds, error rates) | Diagnose and understand **unknown** issues by analyzing all available telemetry |
| **Data Focus** | Primarily predefined metrics and simple logs                    | All telemetry (metrics **＋** rich logs **＋** traces) for deep analysis          |
| **Approach**   | Set static alerts and dashboards                                | Use data correlation and flexible queries to investigate                        |
| **Example**    | Alert if HTTP error rate exceeds 5%                             | Trace a slow API call through multiple services to find the root cause          |

### Why Observability Matters

Modern applications are distributed across microservices and containers, making debugging without comprehensive telemetry difficult. Observability lets teams detect anomalies (e.g. throughput drops), identify patterns (e.g. memory leaks over time), and even predict issues before they impact users. With proper observability, developers build resilient systems that can be monitored throughout the CI/CD lifecycle.

**Quiz & Discussion**

1. **Multiple Choice:** Which of the following is **NOT** a primary pillar of observability?
   A. Logs  B. Metrics  C. Traces  D. Backups
   **Answer:** D. Backups (observability focuses on logs, metrics, and traces).

2. **True/False:** “Monitoring is a subset of observability.”
   **Answer:** True. Observability includes monitoring, but adds deeper analysis capabilities.

3. **Short Answer:** *Explain the difference between monitoring and observability in your own words.*
   **Answer:** Monitoring answers **“what and when”** (e.g. alert on high latency), whereas observability answers **“why and how”** by correlating logs, metrics, and traces to diagnose issues.

4. **Short Answer:** *Name the three pillars of observability.*
   **Answer:** Metrics, Logs, and Traces.

---

## Module 2: Telemetry Signals

In this module we delve into the three telemetry signal types with examples and real-world use-cases.

### 2.1 Metrics

**Definition:** Metrics are numerical measurements recorded over time. In Prometheus’s terms, they are *time series* data with (timestamp, value, and labels). For example, a web server might expose metrics like `http_requests_total` or `response_latency_seconds`. Metrics answer questions such as *“How many requests per second?”* or *“Is CPU usage rising?”*.

**Real-World Example:** Suppose a web application becomes slow. You might look at metrics like request rate and error rate. If `http_requests_total` (the number of requests) spikes, that could explain higher load; if CPU usage is near 100%, that could signal resource contention.

**How Metrics Help:** As the Prometheus docs note, metrics are crucial to understand performance issues. For instance, if a web app slows down under high load, a `request_count` metric lets you correlate load to performance. By alerting on metric thresholds or graphing trends, teams can maintain system health proactively.

### 2.2 Logs

**Definition:** Logs are records of discrete events or messages generated by applications or infrastructure components. They are typically timestamped text entries (e.g. error messages, transaction logs). Logs are unstructured or semi-structured and give detailed context—**“why”** something happened. For example, an application log might show a stack trace or an exception message.

**Real-World Example:** In a login service, a metric might show an increase in failed logins. To find **why** it’s happening, you check the logs: perhaps you discover messages like `UserAuthenticationException: invalid credentials`, indicating a bug or bad input.

**How Loki Fits In:** Grafana Loki is a log aggregation system **inspired by Prometheus**. It is horizontally scalable and multi-tenant. Unlike full-text indexing, Loki only indexes *labels* (metadata) for each log stream. This makes it cost-effective for high-volume logs. Typically, you install Loki (e.g. via Helm on Kubernetes), deploy an agent (Grafana Agent or Promtail) to collect logs, and then query logs in Grafana using LogQL.

&#x20;*Figure: Overview of a Loki-based logging pipeline: (1) Install Loki; (2) Deploy Grafana Agent/Promtail to ship logs; (3) Visualize logs in Grafana; (4) Query logs with LogQL.*

**Example Workflow:** To collect logs with Loki, one would typically:

1. **Install Loki** (e.g. on Kubernetes using the official Helm chart).
2. **Deploy a log shipper** (Grafana Agent or Promtail) to your application hosts or pods. Configure it to tail log files or streams and attach useful *labels* (e.g. `app=frontend`, `env=prod`).
3. **Set up Grafana** and add Loki as a data source. Use Grafana’s *Explore* feature to run queries in **LogQL** (Loki’s query language). For example, to find errors from the “frontend” app, you might run:
   `{app="frontend"} |= "ERROR"`
   (This selects log entries where the label `app="frontend"` and the log line contains `"ERROR"`.)

Logs are invaluable for forensic analysis. For instance, after a deployment, logs might reveal configuration errors or stack traces that metrics alone cannot. Combining logs with metrics and traces enables complete observability.

### 2.3 Traces

**Definition:** Traces (or distributed traces) capture the flow of individual requests as they propagate through a system. A trace consists of multiple *spans*, each representing work done by a service or component. Tracing answers **“how”** a request was handled end-to-end. For example, a trace might show that an HTTP request to Service A called Service B, which in turn queried a database, with timing breakdowns for each call.

**Real-World Example:** Imagine a user’s request to load a web page is slow. Distributed tracing might reveal that Service A took 50ms, but the external API call it made took 300ms. Knowing this, engineers can focus on that API call.

**OpenTelemetry:** OpenTelemetry is a vendor-neutral framework for collecting traces (as well as metrics and logs). It provides APIs and SDKs for many languages to instrument applications, and a **Collector** to receive and export telemetry. For example, you might add the OpenTelemetry SDK to a Python service. When it handles a request, the SDK creates spans and sends them (via the OTLP protocol) to an OpenTelemetry Collector or directly to a backend like Jaeger or Zipkin. The Collector can also ingest Prometheus metrics and other formats, unifying telemetry pipelines.

&#x20;*Figure: OpenTelemetry reference architecture. Applications are instrumented to emit telemetry (traces, metrics, logs) which the OpenTelemetry Collector can receive, process (e.g. batch, sample, enrich), and export to backends (e.g. Prometheus, Jaeger, Splunk).*

**Example Workflow:** A common tracing setup is: instrument your code with OpenTelemetry (or automatic instrumentation), run the **OpenTelemetry Collector** as a sidecar or service, and configure it to export traces to a tracing system. For example, you could export to Grafana Tempo or Jaeger for visualization. This lets you chart the full call path of requests and measure latencies at each step.

### Quiz: Telemetry Signals

1. **Multiple Choice:** Which telemetry type is best for viewing overall system load trends over time?
   A. Logs   B. Metrics   C. Traces   D. Alerts
   **Answer:** B. Metrics. Metrics are time-series numbers (e.g. CPU%, request rate) used to monitor trends.

2. **Multiple Choice:** Grafana Loki primarily indexes **A) the full text of logs**, **B) log labels/metadata**, **C) no data (it’s pull-based)**, **D) only metrics**.
   **Answer:** B. Loki indexes labels for each log stream instead of full text.

3. **True/False:** “A distributed trace shows the path and timing of a single user request across services.”
   **Answer:** True. Traces consist of spans that illustrate how a request flows through the system.

4. **Short Answer:** *Give an example of a LogQL query in Grafana Loki and explain what it does.*
   **Answer:** Example: `` `{app="frontend"} |= "error"` ``. This selects all log entries where the label `app` equals `"frontend"` **and** the log message contains the substring `"error"`. It filters by metadata and message content.

5. **Short Answer:** *Name two open-source tools for distributed tracing.*
   **Answer:** Jaeger and Zipkin (among others). OpenTelemetry can export to them.

---

## Module 3: Prometheus (Metrics Observability)

Prometheus is a popular open-source toolkit for **metrics** collection and alerting. It uses a **pull** model: a central Prometheus server *scrapes* HTTP endpoints of applications or exporters to gather metrics. All data is stored as labeled time series locally.

**Key Features:** Prometheus offers a multi-dimensional data model (metrics named plus key/value labels). It provides **PromQL**, a powerful query language to aggregate and analyze metrics. Prometheus servers are standalone and do **not rely on external storage** for operation, making them reliable during outages. (A Pushgateway is available for short-lived jobs if push is needed.) Alerts can be defined via **Alertmanager**, and data can be visualized with tools like Grafana.

&#x20;*Figure: Prometheus architecture. The Prometheus server scrapes instrumented applications (directly or via Pushgateway), stores time-series data locally, evaluates alert rules, and exposes data to visualization tools like Grafana.*

**Prometheus Workflow Example:**

1. **Instrument Applications:** Use a Prometheus client library (for Go, Python, Java, etc.) to expose an HTTP `/metrics` endpoint. The library lets you define counters, gauges, histograms, etc. For example, you might do:

   ```go
   httpRequests := prometheus.NewCounterVec(prometheus.CounterOpts{
       Name: "http_requests_total", Help: "Count of HTTP requests",
   }, []string{"path"})
   prometheus.MustRegister(httpRequests)
   ```
2. **Configure Prometheus:** In `prometheus.yml`, add a `scrape_config` pointing to your service. Prometheus will periodically query `/metrics`. For instance:

   ```yaml
   scrape_configs:
     - job_name: 'my-app'
       static_configs:
         - targets: ['localhost:9000']
   ```
3. **Run Prometheus:** Start the server. It will discover the `/metrics` endpoints and store all scraped samples with timestamps and labels.
4. **Query and Alert:** Use the Prometheus expression browser or Grafana to write queries. Example: `rate(http_requests_total[5m])` gives the per-second request rate over 5 minutes. You can define an alert like `alert: HighErrorRate` when `increase(error_total[5m]) > 100`. Alerts are handled by Alertmanager for notifications.

**When to Use Prometheus:** It excels at *machine-centric numeric monitoring*. Its multi-dimensional model is ideal for dynamic environments. The docs note that Prometheus “works well for any purely numeric time series” and is designed for reliability even if other systems fail. However, if you need absolutely 100% accurate event logging (e.g. billing), Prometheus may not suit that use-case.

**Quiz: Prometheus**

1. **Multiple Choice:** Prometheus collects metrics using which model? A) **Pull** via HTTP endpoints, B) Push to server, C) Writes directly to database, D) Scans logs.
   **Answer:** A. Pull. Prometheus scrapes endpoints over HTTP (with push via gateway as an option).

2. **Multiple Choice:** Prometheus stores metrics as:
   A) Raw logs, B) Relational DB, **C) Time series with labels**, D) CSV files.
   **Answer:** C. Time series (each with timestamp, value, and labels).

3. **True/False:** “Prometheus requires a central database like MySQL to store data.”
   **Answer:** False. Prometheus stores data in its own time-series database on disk.

4. **Short Answer:** *What is PromQL and why is it useful?*
   **Answer:** PromQL is Prometheus’s query language for slicing and aggregating time-series data. It lets you compute rates, histograms, sums, etc. over metrics for monitoring and alerting (e.g. `sum(rate(http_requests_total[5m])) by (service)`).

5. **Exercise (Hands-On):** Install Prometheus (e.g. via Docker or Kubernetes). Use the Node Exporter to collect host metrics. Configure Prometheus to scrape Node Exporter and Grafana for visualization. Verify you can graph CPU or memory usage in Grafana.

---

## Module 4: Grafana Loki (Logging Observability)

Grafana Loki is a log aggregation system designed to work with Prometheus-style labeling. Unlike systems that index full log text, Loki **indexes only the labels** (such as `app`, `cluster`, `env`) of log streams, which makes it very efficient and scalable. Loki is **horizontally scalable** and **multi-tenant**, enabling centralized log collection for large environments.

**Getting Started:** To collect logs with Loki, one typically installs Loki (often via Helm on Kubernetes) and deploys a log shipper like **Grafana Agent** or **Promtail**. The shipper agents can tail logs from Kubernetes pods or file systems, add metadata labels (e.g. service name, instance), and send log streams to Loki. In Grafana, you add Loki as a data source and use the *Explore* section to query logs using **LogQL**.

**LogQL Queries:** LogQL is a powerful query language similar in spirit to PromQL, allowing **filtering and parsing of logs**. For example:

```logql
{app="web"} |= "timeout" | json | sum by (status_code) (count_over_time({app="web"}[5m]))
```

This query finds logs where `app="web"` and the line contains `"timeout"`, parses them as JSON, and counts occurrences by HTTP status code over the last 5 minutes.

**Example Workflow:** Following the steps in the Loki docs:

1. **Install Loki:** Use the Helm chart with your preferred storage backend (e.g. AWS S3).
2. **Deploy Grafana Agent:** Configure it to *discover* Kubernetes pods and ship their logs to Loki. Include labels like `container` and `pod` for context.
3. **Add Grafana Dashboard:** Create a Loki data source in Grafana. In *Explore*, run a query such as `{namespace="prod", app="frontend"} |= "ERROR"` to see all error logs from the frontend app.

**Use Case:** Loki is especially effective when you want to query logs by **service or region** without the cost of full-text search. It pairs well with Prometheus; you can alert on a metric spike (e.g. error rate) and then jump to the logs in Grafana to investigate details.

**Quiz: Loki**

1. **Multiple Choice:** Grafana Loki differs from Elasticsearch in that it:
   A) indexes only metadata labels, not full log text,
   B) is only for metrics,
   C) requires proprietary hardware,
   D) cannot filter queries.
   **Answer:** A. Loki indexes only labels and fetches log contents on demand.

2. **True/False:** “Grafana Agent can be used to collect logs for Loki.”
   **Answer:** True. Grafana Agent (and Promtail) are recommended tools to ship logs to Loki.

3. **Short Answer:** *What is LogQL and how does it relate to Loki?*
   **Answer:** LogQL is Loki’s query language for logs (similar to Prometheus’s PromQL). It allows filtering log streams by label and content. For example, `{app="backend"} |= "error"` finds log lines containing “error” for the `backend` app.

4. **Exercise (Hands-On):** Deploy a Loki stack (Loki + Grafana Agent). Configure it to collect logs from a sample application (e.g. Nginx or a custom app). Create a Grafana dashboard and use LogQL to filter logs (e.g. show only “error” messages or logs for a specific pod).

---

## Module 5: OpenTelemetry (Instrumentation & Tracing)

OpenTelemetry (OTel) is a vendor-neutral, open-source framework for **instrumenting**, **generating**, and **exporting** telemetry data (traces, metrics, and logs). It arose from the merger of OpenTracing and OpenCensus projects. As an industry standard, it integrates with 40+ vendors and is widely supported.

**Key Components:** The main parts of OpenTelemetry include:

* **API and SDKs:** Language-specific libraries (Java, Python, Go, etc.) for manual or automatic instrumentation. These let applications create traces, metrics, and logs, and export them.
* **Instrumentation Libraries:** Pre-built libraries for popular frameworks (e.g. HTTP, databases) so you can auto-generate spans for incoming/outgoing calls.
* **Collector:** A standalone service (the OpenTelemetry Collector) that can receive telemetry in various formats (OTLP, Jaeger, Prometheus, etc.), process it (batching, sampling), and export to backends.
* **Exporters:** Components that send data to observability backends. For example, you can export traces to Jaeger or metrics to Prometheus. The OpenTelemetry Protocol (OTLP) is the native format for lossless transmission and is supported by many tools.

&#x20;*Figure: Components of OpenTelemetry. Applications instrumented with OTel emit telemetry to an OTel Collector, which can process and export data to various backends (e.g. Prometheus, Jaeger, Splunk).*

**Instrumentation Example:** To collect traces and metrics from your code:

1. **Add OTel SDK:** In your service (e.g. a Python Flask app), install the OpenTelemetry SDK and auto-instrumentor. For instance:

   ```bash
   pip install opentelemetry-sdk opentelemetry-instrumentation-flask
   otel-instrument python app.py
   ```
2. **Configure the Collector:** Run the OpenTelemetry Collector (as a separate process or Kubernetes deployment). Configure it to receive OTLP data and export to a backend, e.g. Jaeger or Grafana Tempo.
3. **Send Data:** The app will generate spans and metrics and send them via gRPC/HTTP to the Collector (or directly to a backend).
4. **Visualize:** Use Jaeger UI or Grafana tracing plugins to view traces and latency breakdowns.

**OpenTelemetry vs Others:** Because it is vendor-neutral, you can switch exporters without changing application code. For example, you could send metrics to Prometheus and traces to Jaeger from the same instrumentation. It’s a common approach to use OTel SDKs and Collector in production for a unified observability pipeline.

**Quiz: OpenTelemetry**

1. **Multiple Choice:** OpenTelemetry is: A) A proprietary vendor, B) A logging format, C) A vendor-neutral telemetry framework, D) A database.
   **Answer:** C. A vendor-neutral open-source observability framework.

2. **Multiple Choice:** The OpenTelemetry Collector can receive data in which formats? (Select all that apply)
   A) OTLP (OpenTelemetry Protocol), B) Prometheus scraping (exporter), C) Jaeger, D) FTP.
   **Answer:** A, B, C. It supports OTLP, Prometheus, Jaeger, Zipkin, and more.

3. **True/False:** “OpenTelemetry requires all data to go through its Collector; it cannot export directly to backends.”
   **Answer:** False. While the Collector is recommended, OTel SDKs can export directly to backends (e.g. directly to Jaeger or Prometheus).

4. **Short Answer:** *What is an exporter in OpenTelemetry? Give an example.*
   **Answer:** An exporter is a component that sends telemetry data to a specific backend. For example, the OTel Prometheus exporter pushes metrics to a Prometheus server; the Jaeger exporter sends traces to a Jaeger backend.

5. **Exercise (Hands-On):** Instrument a sample application with OpenTelemetry (choose any language SDK). Run the OpenTelemetry Collector and export data to a backend: e.g., Prometheus for metrics and Jaeger for traces. Use Grafana to visualize metrics and traces.
